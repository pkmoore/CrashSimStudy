\section{Introduction}
\label{SEC:introduction}
\textit{No Battle Plan Survives Contact With the Enemy --- Helmuth von Moltke}

No matter how well an application is tested before being released, new bugs
always seem to be found after deployment. An important reason explaining
this observation is that applications are running in a diverse set of
different deployment \emph{environments}. Many applications seek to work
across different operating systems, network types, etc.  Changing the
operating system can cause new bugs to occur~\cite{LinuxGlibcChanges}, file
systems exhibit subtle but critical differences among each
other~\cite{EXT4Layout, AppleHFS}, and differences in network behavior can
be substantial, even in situations where the network type or even the
adapter are identical~\cite{vbox}. These environmental differences greatly
exacerbate the problems in ensuring that an application functions correctly
before it is deployed.

A recent survey conducted by ClusterHQ~\cite{ClusterHQSurvey} confirmed
that application developers spend a significant portion of their time
debugging errors that are only discovered in production.  The survey
participants cited the inability to recreate production environments for
testing as the main reason why bugs are not discovered earlier.  Putting
forward enormous developer effort may be insufficient to uncover these bugs
before deployment.  Microsoft employs thousands of engineers with nearly a
1:1 ratio of testers to developers~\cite{Page2009}.  In spite of this
enormous emphasis on testing a recent Windows Update released in response
to the Spectre Intel CPU vulnerability resulted in machines with certain
hardware configurations being rendered unbootable~\cite{kb4056892}.  Even
specialized ``Write-Once, Run Anywhere'' environments that attempt to hide
these environmental differences, such as the Java Runtime Environment, are
not perfect, leading them to be rechristened ``Write-Once, Debug
Everywhere''~\cite{WODE}.  Clearly, bugs due to differences between
development and production environments are being missed in testing only to
be found in deployment.

In this paper, we introduce {\em CrashSimulator}\footnote{ Our approach is
loosely inspired by flight simulators, which test pilot aptitude under a
variety of rare, adverse scenarios (water landings, engine failures, etc.)
before the pilots are certified to work in practice.}, a testing approach
and tool that can test an application as though it is running in a large
number of diverse environments likely to cause crashes.  It is not feasible
to test all possible environments, so an important consideration is how to
choose which environments to test.  These environments can be chosen by
identifying situations that caused environmental bugs in other applications
and then testing for them systematically.  Selecting situations in this way
means that an environmental bug that is detected in one application can be
detected by CrashSimulator in other applications.  This merely requires
CrashSimulator be configured to identify the environmental bug in question.
Once this is done, any application can be tested for the bug using
CrashSimulator with no added effort.  CrashSimulator can be configured to
test a suite of bugs accumulated from a set of applications running in a
variety of environments.  This is somewhat analogous to CrashSimulator
automatically creating unit tests for all environments in which an
application could run.  Each of these ``tests'' consists of rules for
mutating system call behavior to give the application the illusion that it
is executing in an anomalous environment and rules for determining whether
or not the application responds appropriately.  Because these rules can be
applied to many applications that may be prone to mishandling the anomalous
environment in question, prior knowledge gathered from past deployment
experiences can be used to more thoroughly work out a new application so
that it can handle the challenges of its target environments correctly.

CrashSimulator's technique for performing this testing procedure is based
on the insight that environmental anomalies can be represented as anomalies
in results and side effects of the system calls an application makes.
CrashSimulator tests an application by exposing it to these anomalous
conditions during the course of execution and evaluating the application's
response to them.  The details of this exposure are configured by taking a
system call trace of the application under normal conditions and modifying
it such that the anomalous conditions are represented.  CrashSimulator uses
these modified system call traces to control a replay execution of the
application in which these conditions will be encountered.

In the course of this work we built a proof of concept version of
CrashSimulator's technique that was able to find bugs, both known and
unknown, in popular Linux applications as ranked by Debian's popularity
contest~\cite{DebPopCon}.  These bugs were found by exposing the
applications to environmental conditions that simulate unusual file system
configurations, file types, and network delays.  When the applications in
question were actually exposed to these conditions a variety of failures
including hangs, crashes, and filesystem damage occurred.  In total, 84
bugs were identified.

In addition to these quantitative results, this paper presents the results
of a user study conducted with the goal of determining how CrashSimulator
performs in the hands of developers as they test real world applications.
This study involved ZZZ participants consisting of Master's computer
science students with varying backgrounds and specializations.  These
participants were asked to test existing popular applications (as ranked by
Debian's Popularity Contest) using CrashSimulator, AFL, and Mutiny.  This
survey yielded results in the form of numbers of bugs identified and
qualitative results about the tools' user experience through surveys.
These results show that developers are able to find new bugs with
CrashSimulator, many of which would not have been found with the other
tools due to its unique testing approach.  Additionally, survey
participants report that CrashSimulator allows them to find bugs they would
not have been about to find otherwise due to a self-reported lack of
experience with operating systems concepts.

With both sets of result in mind, the main contributions in this work are
as follows:

\begin{enumerate}

\item{It proves the importance of the interaction between an application
and its environment in creating potential flaws upon deployment.}

\item{It proposes the idea that by manipulating these interactions through
the use of system calls, the responses of an application to any given
environment can be accurately simulated without an actual
deployment to that environment.}

\item{It offers an approach for encoding an appropriate flow of
interactions between an application and its environment as a model that
can be later used to check for correct behavior.}

\item{It shows that CrashSimulator allows these developers to find real
bugs in real applications.}

\item{It provides proof that CrashSimulator compares favorably against
similar automated testing tools in the number and type of bugs found.}

\item{It submits favorable opinion results taken from study participant
survey data.}

\end{enumerate}
