\section{Introduction}
\label{SEC:introduction}
\textit{No Battle Plan Survives Contact With the Enemy --- Helmuth von Moltke}

No matter how well an application is tested before being released, new bugs
always seem to be found after deployment. An important reason explaining
this observation is that applications are running in a diverse set of
different deployment \emph{environments}. Many applications seek to work
across different operating systems, network types, etc.  Changing the
operating system can cause new bugs to occur~\cite{LinuxGlibcChanges}, file
systems exhibit subtle but critical differences among each
other~\cite{EXT4Layout, AppleHFS}, and differences in network behavior can
be substantial, even in situations where the network type or even the
adapter are identical~\cite{vbox}. These environmental differences greatly
exacerbate the problems in ensuring that an application functions correctly
before it is deployed.

A recent survey conducted by ClusterHQ~\cite{ClusterHQSurvey} confirmed
that application developers spend a significant portion of their time
debugging errors that are only discovered in production.  The survey
participants cited the inability to recreate production environments for
testing as the main reason why bugs are not discovered earlier.  Putting
forward enormous developer effort may be insufficient to uncover these bugs
before deployment.  Microsoft employs thousands of engineers with nearly a
1:1 ratio of testers to developers~\cite{Page2009}.  In spite of this
enormous emphasis on testing a recent Windows Update released in response
to the Spectre Intel CPU vulnerability resulted in machines with certain
hardware configurations being rendered unbootable~\cite{kb4056892}.  Even
specialized ``Write-Once, Run Anywhere'' environments that attempt to hide
these environmental differences, such as the Java Runtime Environment, are
not perfect, leading them to be rechristened ``Write-Once, Debug
Everywhere''~\cite{WODE}.  Clearly, bugs due to differences between
development and production environments are being missed in testing only to
be found in deployment.

In this paper, we introduce {\em CrashSimulator}\footnote{ Our approach is
loosely inspired by flight simulators, which test pilot aptitude under a
variety of rare, adverse scenarios (water landings, engine failures, etc.)
before the pilots are certified to work in practice.}, a testing approach
and tool that can test an application as though it is running in a large
number of diverse environments likely to cause crashes.  It is not feasible
to test all possible environments, so an important consideration is how to
choose which environments to test.  These environments can be chosen by
identifying situations that caused environmental bugs in other applications
and then testing for them systematically.  Selecting situations in this way
means that an environmental bug that is detected in one application can be
detected by CrashSimulator in other applications.  This merely requires
CrashSimulator be configured to identify the environmental bug in question.
Once this is done, any application can be tested for the bug using
CrashSimulator with no added effort.  CrashSimulator can be configured to
test a suite of bugs accumulated from a set of applications running in a
variety of environments.  This is somewhat analogous to CrashSimulator
automatically creating unit tests for all environments in which an
application could run.  Each of these ``tests'' consists of rules for
mutating system call behavior to give the application the illusion that it
is executing in an anomalous environment and rules for determining whether
or not the application responds appropriately.  Because these rules can be
applied to many applications that may be prone to mishandling the anomalous
environment in question, prior knowledge gathered from past deployment
experiences can be used to more thoroughly work out a new application so
that it can handle the challenges of its target environments correctly.

CrashSimulator's technique for performing this testing procedure is based
on the insight that environmental anomalies can be represented as anomalies
in results and side effects of the system calls an application makes.
CrashSimulator tests an application by exposing it to these anomalous
conditions during the course of execution and evaluating the application's
response to them.  The details of this exposure are configured by taking a
system call trace of the application under normal conditions and modifying
it such that the anomalous conditions are represented.  CrashSimulator uses
these modified system call traces to control a replay execution of the
application in which these conditions will be encountered.

In the course of this work we built a proof of concept version of
CrashSimulator's technique that was able to find bugs, both known and
unknown, in popular Linux applications as ranked by Debian's popularity
contest~\cite{DebPopCon}.  These bugs were found by exposing the
applications to environmental conditions that simulate unusual file system
configurations, file types, and network delays.  When the applications in
question were actually exposed to these conditions a variety of failures
including hangs, crashes, and filesystem damage occurred.  In total, 84
bugs were identified.

Yet, designing and carrying out this testing must be done carefully as it
presents a real chance of introducing new inefficiencies and pain points
into a project's development life cycle.  Therefore, there are four
critical variables that need to be considered.
First, reliance on experts and
human-in-the-loop strategies do not scale.\preston{cite Alvaro}  There are
simply not enough testing experts to go around,
and a developer's time is far
better spent doing actual development than manual testing.\preston{cite
developer shortages}  Fortunately, by using well constructed automated
testing tools, project teams can kill two birds with one proverbial stone;
such tools can free up developer time for more creative tasks while taking
advantage of the expert knowledge encoded in the tools to temper the
shortage of human experts.

\preston{Discuss what properties an automated testing tool must have}

The second point is that one can not simply grab any tool off the shelf (or
internet).  Any selected tool must capable of testing the system in
question, be usable by the developers on staff, and produce
high-quality results.  Related to this is a fourth consideration: in
addition to a general shortage of development capacity, one must consider
each developer's skill sets and expertise will vary.
In order to be successful, a
testing tool must be usable by all developers working on a system. If
lack of the necessary background means a developer can not
deal with a tool's output,
then any effort in learning how to implement the tool or  evaluate its
results will be wasted.

The fourth and final variable is the quality of the tool's output.  Tools
that produce a high number of false positives are quickly deemed
untrustworthy.  A study by ZZZZ has shown that false positive rates as low
as Q\% have a dramatic, negative impact on developer perceptions of a tool.
\preston{cite}  A similar finding is supported by current psychological
literature.  Individuals tend to over-emphasize and over-predict
statistically rare negative events.\preston{cite}  Put another way, if a
tool leads to a developer wasting his or her afternoon by reporting a bug
that doesn't exist, the developer is going to mistrust that tool from then
on out.

\preston{not sure how to word this given that this might be the second
paper}


To validate these claims we conducted a study with ZZZ participants
consisting of Master's computer science students with varying backgrounds
and specializations.  We asked these participants to test existing popular
applications (as ranked by Debian's Popularity Contest) using each of the
tools.  From this work we collected both quantitative results in terms of
numbers of bugs identified, and qualitative results about the tools' user
experience through surveys.

The main contributions in this work are as follows:

\begin{enumerate}

\item{It proves the importance of the interaction between an application
and its environment in creating potential flaws upon deployment.}

\item{It proposes the idea that by manipulating these interactions through
the use of system calls, the responses of an application to any given
environment can be accurately simulated without an actual
deployment to that environment.}

\item{It offers an approach for encoding an appropriate flow of
interactions between an application and its environment as a model that
can be later used to check for correct behavior.}

\item{It shows that CrashSimulator allows these developers to find real bugs
in real applications.}

\item{It provides proof that CrashSimulator compares favorably against similar
automated testing tools in the number of bugs found.}

\item{It submits favorable opinion results taken from study participant survey
data.

\end{enumerate}
