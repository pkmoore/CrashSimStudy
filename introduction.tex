\section{Introduction}
\label{SEC:introduction}
\textit{No Battle Plan Survives Contact With the Enemy --- Helmuth von Moltke}

No matter how well an application is tested before being released, new bugs
always seem to be found after deployment.  One reason for this situation
is that applications run in a diverse set of
different deployment \emph{environments}, - and often
across different operating systems, and network types,
each of which can introduce new flaws~\cite{LinuxGlibcChanges}.
In addition, file
systems can exhibit subtle but critical
differences~\cite{EXT4Layout, AppleHFS}, and while differences in network
behavior can
be substantial, even if the network type or even the
adapter are identical~\cite{vbox}. These environmental differences greatly
exacerbate the chance of ensuring that an application will function
correctly when deployed.

This complicates the work of application developers who, according to a
recent survey conducted by ClusterHQ~\cite{ClusterHQSurvey},
spend a significant portion of their time
debugging errors that only appear in production.
Participants in this survey cited the inability to recreate
production environments for
testing as the main reason why bugs are not discovered earlier.
Even if an
enormous developer effort is put forward, it may be insufficient
to uncover these bugs
before deployment.  Microsoft employs thousands of engineers with nearly a
1:1 ratio of testers to developers~\cite{Page2009}.
Yet, recent Windows Update released in response
to the Spectre Intel CPU vulnerability resulted in machines with certain
hardware configurations were rendered unbootable~\cite{kb4056892}.  Even
specialized ``Write-Once, Run Anywhere'' environments that attempt to hide
these environmental differences, such as the Java Runtime Environment, are
not perfect, leading them to be rechristened ``Write-Once, Debug
Everywhere''~\cite{WODE}.

In this paper, we introduce {\em CrashSimulator}\footnote{ Our approach is
loosely inspired by flight simulators, which test pilot aptitude under a
variety of rare, adverse scenarios (water landings, engine failures, etc.)
before the pilots are certified to work in practice.}, a testing approach
and tool that can replicate a large
number of diverse environments likely to cause crashes before an
application is deployed.  CrashSimulator identifies
situations that have caused environmental bugs in other applications,
and then systematically tests for them.

% Selecting situations in this way
% means that an environmental bug that is detected in one application can be
% detected by CrashSimulator in other applications.  This merely requires
% CrashSimulator be configured to identify the environmental bug in question.
% Once this is done, any application can be tested for the bug using
% CrashSimulator with no added effort.
% 
% CrashSimulator can be configured to
% test a suite of bugs accumulated from a set of applications running in a
% variety of environments.  This is somewhat analogous to CrashSimulator
% automatically creating unit tests for all environments in which an
% application could run.  Each of these ``tests'' consists of rules for
% mutating system call behavior to give the application the illusion that it
% is executing in an anomalous environment and rules for determining whether
% or not the application responds appropriately.  Because these rules can be
% applied to many applications that may be prone to mishandling the anomalous
% environment in question, prior knowledge gathered from past deployment
% experiences can be used to more thoroughly work out a new application so
% that it can handle the challenges of its target environments correctly.

CrashSimulator's technique for performing this testing procedure is based
on the insight that environmental anomalies can be represented as anomalies
in results and side effects of the system calls an application makes.
CrashSimulator tests an application by exposing it to these anomalous
conditions during the course of execution and evaluating the application's
response to them.  The details of this exposure are configured by taking a
system call trace of the application under normal conditions and modifying
it such that the anomalous conditions are represented.  CrashSimulator uses
these modified system call traces to control a replay execution of the
application in which these conditions will be encountered.

In the course of this work we built a proof of concept version of
CrashSimulator's technique that was able to find bugs, both known and
unknown, in popular Linux applications as ranked by Debian's popularity
contest~\cite{DebPopCon}.  These bugs were found by exposing the
applications to environmental conditions that simulate unusual file system
configurations, file types, and network delays.  When the applications in
question were actually exposed to these conditions a variety of failures
including hangs, crashes, and filesystem damage occurred.  In total, 84
bugs were identified.

In addition to these quantitative results, this paper presents the results
of a user study conducted with the goal of determining how CrashSimulator
performs in the hands of developers as they test real world applications.
This study involved ZZZ participants consisting of Master's computer
science students with varying backgrounds and specializations.  These
participants were asked to test existing popular applications (as ranked by
Debian's Popularity Contest) using CrashSimulator, AFL, and Mutiny.  This
survey yielded results in the form of numbers of bugs identified and
qualitative results about the tools' user experience through surveys.
These results show that developers are able to find new bugs with
CrashSimulator, many of which would not have been found with the other
tools due to its unique testing approach.  Additionally, survey
participants report that CrashSimulator allows them to find bugs they would
not have been about to find otherwise due to a self-reported lack of
experience with operating systems concepts.

With both sets of result in mind, the main contributions in this work are
as follows:

\begin{enumerate}

\item{It proves the importance of the interaction between an application
and its environment in creating potential flaws upon deployment.}

\item{It proposes the idea that by manipulating these interactions through
the use of system calls, the responses of an application to any given
environment can be accurately simulated without an actual
deployment to that environment.}

\item{It offers an approach for encoding an appropriate flow of
interactions between an application and its environment as a model that
can be later used to check for correct behavior.}

\item{It shows that CrashSimulator allows these developers to find real
bugs in real applications.}

\item{It provides proof that CrashSimulator compares favorably against
similar automated testing tools in the number and type of bugs found.}

\item{It submits favorable opinion results taken from study participant
survey data.}

\end{enumerate}
