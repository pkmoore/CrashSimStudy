\section{Introduction}
\label{SEC:introduction}
\textit{No Battle Plan Survives Contact With the Enemy --- Helmuth von Moltke}

No matter how well an application is tested before its release,
new bugs always seem to emerge after deployment.
In fact, Oracle estimates that 40\% of deployed applications
contain critical defects -- a situation that is compounded
by the fact that deployment
increases the cost to fix these flaws by 100 times~\cite{OracleAppQuality}.
One reason for this behavior
is that these applications will operate within a diverse set of
deployment \emph{environments},
and variations between these environments tend to
reveal previously undiscovered flaws.
These flaws emerge from
such factors as
operating system APIs changing across versions
~\cite{LinuxGlibcChanges},
or small variations in file systems exhibiting subtle but critical
differences~\cite{EXT4Layout, AppleHFS}.
Even if the network and adapter are identical,
network behavior can still diverge from what is expected~\cite{vbox},
and these environmental differences greatly exacerbate
the chance that an application will function incorrectly when deployed.

These unforeseen bugs
complicate the work of 43\% of application developers who, according to a
recent survey conducted by ClusterHQ~\cite{ClusterHQSurvey},
spend between 10\% and 25\% of their time
debugging errors that only appear in production.
%Participants in this survey cited the inability to recreate
%production environments for
%testing as the main reason why bugs are not discovered earlier.
Numerous efforts have been made to reduce this burden.
One approach
is to hide environmental differences behind standard interfaces.
Unfortunately,
even specialized ``Write-Once, Run Anywhere'' environments
that attempt to hide these differences,
such as the Java Runtime Environment,
are not perfect,
leading them to be rechristened ``Write-Once, Debug Everywhere''~\cite{WODE}.
A more direct approach would be
to identify and fix deficiencies before deployment,
but history has shown that,
even if enormous effort is put forward,
it may be insufficient to uncover these bugs.
Microsoft employs thousands of engineers with nearly a
1:1 ratio of testers to developers~\cite{Page2009}.
Yet, a recent Windows Update released
in response to the Spectre Intel CPU vulnerability
resulted in machines with certain hardware configurations
being rendered unbootable~\cite{kb4056892}.

What is needed is a methodical way to record and preserve specific features of any environment proven to have caused an incorrect response in applications. By recording,
storing, and cataloging these features, which we call \textit{anomalies}, we offer developers a
systematic and reproducible strategy for future application tests, without requiring per application effort. This is an improvement over current practices, where findings are often lost or, at best, preserved as institutional knowledge that may not survive developer turnover.

In  this paper, we document the development and implementation of a new
approach to finding and preserving bugs that we call \textit{Simulating
Environmental Anomalies}(SEA). This technique is founded upon the key
insight that problematic environmental properties can often be detected
in the function calls, system
calls, or other communications an application makes within an
environment. When employing SEA, anomalies unique to a given environment
can be inserted into the communications of
an application under test in such a way that its responses will indicate
potential for failures upon deployment. In this way, developers are given
an easy and inexpensive way to learn from the mistakes of others, and
thus save money and programming hours that otherwise would be spent to
find and fix environmental bugs.

Tests of this technique, conducted in the form of a proof of concept tool
called {\em CrashSimulator}\footnote{Our approach is
loosely inspired by flight simulators, which test pilot aptitude under a
variety of rare, adverse scenarios (water landings, engine failures,
etc.) before the pilots are certified to work in practice.}, we were
able to find bugs, both known and unknown, in Linux applications ranked
highly on Debian’s popularity contest~\cite{DebPopCon}. The tool proved
its efficacy in identifying environmental anomalies attributed to
unanticipated file system configurations, file types, and network delays
that can cause a variety of failures, including hangs, crashes, and
filesystem damage.  In total, the SEA technique was able to identify 65
bugs.  Furthermore, we were able to identify these bugs with much less
time and effort than would be required to set up real environments and
execute the same applications within them, making SEA a more feasible
approach for developers to use in real-world settings.

Another benefit of using the SEA technique is we were able to affirm its
useability, even for those developers with limited experience. Using the
CrashSimulator tool, 12 undergraduate and graduate computer science
students were able to complete tests of the same type of applications
evaluated in our initial experiments. The results show that the
developers were able to find bugs that had been missed by the
applications’ existing testing strategies. Additionally, the participants
were able to find bugs in environments with which they had only a limited
amount of experience.

The main contributions in this work can be summarized as follows:

\begin{itemize}

\item{It provides evidence
that previously unanticipated flaws can be created by the interaction
between an application and its environment.}

\item{It introduces \textit{Simulating Environmental Anomalies} (SEA)
as an easy-to-use method for simulating environments
so an application's behavior in those environments
can be assessed before deployment---
without the time and resource costs of
testing each environment individually.}

\item{It allows developers to build a corpus of extracted anomalies, so
    application testing does not have to rely on trial and error,
    or institutional knowledge,
    that disappears with developer turnover.}

\item{It demonstrates a new tool, {\em CrashSimulator},
which implements SEA
in order to find previously-undiscovered environmental bugs
in widely deployed and highly tested code.}

\item{It proves the effectiveness
of {\em CrashSimulator}
by presenting results
showing developers
can use the tool to find real bugs in real applications.}

\end{itemize}
