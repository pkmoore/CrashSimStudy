\section{Introduction}
\label{SEC:introduction}
\textit{No Battle Plan Survives Contact With the Enemy --- Helmuth von Moltke}

No matter how well an application is tested before its release,
new bugs always seem to emerge after deployment.
One reason for this behavior
is that these applications will operate within a diverse set of
deployment \emph{environments},
and variations between these environments tend to
reveal previously undiscovered flaws.
These flaws emerge from
such factors as
operating system APIs changing across versions
~\cite{LinuxGlibcChanges},
or small variations in file systems exhibiting subtle but critical
differences~\cite{EXT4Layout, AppleHFS}.
Even if the network and adapter are identical,
network behavior can still diverge from what is expected~\cite{vbox},
and these environmental differences greatly exacerbate
the chance that an application will function incorrectly when deployed.

These unforeseeable bugs
complicate the work of application developers who, according to a
recent survey conducted by ClusterHQ~\cite{ClusterHQSurvey},
spend a significant portion of their time
debugging errors that only appear in production.
% Participants in this survey cited the inability to recreate
% production environments for
% testing as the main reason why bugs are not discovered earlier.
Numerous efforts have been made to reduce this burden.
One approach
is to hide environmental differences behind standard interfaces.
Unfortunately,
even specialized ``Write-Once, Run Anywhere'' environments
that attempt to hide these differences,
such as the Java Runtime Environment,
are not perfect,
leading them to be rechristened ``Write-Once, Debug Everywhere''~\cite{WODE}.
A more direct approach would be
to identify and fix deficiencies before deployment,
but history has shown that,
even if enormous effort is put forward,
it may be insufficient to uncover these bugs.
Microsoft employs thousands of engineers with nearly a
1:1 ratio of testers to developers~\cite{Page2009}.
Yet, a recent Windows Update released
in response to the Spectre Intel CPU vulnerability
resulted in machines with certain hardware configurations
being rendered unbootable~\cite{kb4056892}.


The work documented in this paper
seeks to move the needle in the developer's favor
by presenting a new technique for catching environmental bugs
before deployment.
Guiding our efforts
is a technique
we call \textit{Simulating Environmental Anomalies}(SEA).
This technique is founded upon the key insight
that problematic environmental properties,
which we refer to as \textit{anomalies},
can often be detected
in the function calls,
system calls,
or other communications an application makes within an environment.
When employing SEA,
the anomalies
unique to a given environment
can be inserted into
the communications of an application under test
in such a way
that its responses will indicate potential
for failures upon deployment.
In this way,
developers are given
an easy and inexpensive way
to learn from the mistakes of others,
and thus save money and programming hours
that otherwise would be spent to find and fix
environmental bugs.

Furthermore,
the SEA technique
allows developers to preserve and catalog
these anomalies,
giving them a systematic and reproducible strategy for future application
tests.
This is an improvement over current practices,
where findings are often lost or,
at best, preserved as
institutional knowledge
that may not survive developer turnover.
Eventually,
a corpus of environmental anomalies can be assembled
that can help ensure
future applications do not suffer from bugs of the past.

To evaluate the ability of this technique to reveal environmental
bugs, we built and tested a proof of concept tool
called {\em CrashSimulator}\footnote{Our approach is
loosely inspired by flight simulators, which test pilot aptitude under a
variety of rare, adverse scenarios (water landings, engine failures, etc.)
before the pilots are certified to work in practice.}.
Our results showed that the tool was able to find bugs,
both known and unknown,
in Linux applications ranked highly on Debian's popularity
contest~\cite{DebPopCon}.
The applications were exposed
to environmental anomalies present as a result of unanticipated
file system configurations, file types, and network delays.
When the applications in
question were actually exposed to these conditions a variety of failures
including hangs, crashes, and filesystem damage occurred.  In total,
\preston{somenumber}
bugs were identified in this manner.
The effort required to set up real
environments where these conditions are present
and execute the same applications within them
would far exceeded the time required to evaluate them
using simulation -- further illustrating the benefit offered by SEA.

In addition to demonstrating that
CrashSimulator could find bugs, we were also able to
show that developers with varying backgrounds
could use the tool
on real world applications with ease.
We conducted a user study with
12 undergraduate and graduate computer science students
who were asked to use CrashSimulator to test
the same type of applications evaluated in our initial tests.
The results show that the developers were able to find bugs
that were missed by the applications' existing testing strategies.
Additionally, the
participants were able to find bugs
in environments with which they had only a limited amount of experience.

The main contributions in this work can be summarized as follows:

\begin{itemize}

\item{It provides evidence
that previously unanticipated flaws can be created by the interaction
between an application and its environment.}

\item{It introduces \textit{Simulating Environmental Anomalies}
as an easy-to-use method for simulating environments
so an application's behavior in those environments
can be assessed before deployment---
without the time and resource costs of
testing each environment individually.}

\item{It allows developers to build a corpus of extracted anomalies, so
    application testing does not have to rely on trial and error,
    or institutional knowledge,
    that disappears with developer turnover.}

\item{It demonstrates a new tool, {\em CrashSimulator},
which implements SEA
in order to find previously-undiscovered environmental bugs
in widely deployed and highly tested code.}

\item{It proves the effectiveness
of {\em CrashSimulator}
by presenting results
showing developers
could use the tool to find real bugs in real applications.}

\end{itemize}
