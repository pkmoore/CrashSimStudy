\section{Introduction}
\label{SEC:introduction}

% We are evaluating the usability of the tool rather than just the tool
% itself.

% Come up with a way to define the audience of people that can use the tool

% Reference other studies when saying that CrashSimulator does well at
% finding bugs so the purpose of this paper is to find out how usable it
% is.

\preston{CrashSimulator finds these bugs by recording, modifying, and
replaying the results and side effects of the system calls made by the
application.}

\preston{Argue that you have to have automated testing} Automated testing
is a necessary step in developing complex, modern, software systems.  There
are simply too many moving parts to orchestrate for developers to test
product manually.
Industry experience has borne this out --- BBB percent of projects hosted
on GitHub employ some sort of automated testing on code checked into their
repositories and most won't allow code to be checked in until it has passed
some testing regime.\preston{cite}

Yet, designing and carrying out this testing must be done carefully as it
presents a real chance of introducing new inefficiencies and pain points
into a project's development life cycle.  Therefore, there are four
critical variables that need to be considered.
First, reliance on experts and
human-in-the-loop strategies do not scale.\preston{cite Alvaro}  There are
simply not enough testing experts to go around,
and a developer's time is far
better spent doing actual development than manual testing.\preston{cite
developer shortages}  Fortunately, by using well constructed automated
testing tools, project teams can kill two birds with one proverbial stone;
such tools can free up developer time for more creative tasks while taking
advantage of the expert knowledge encoded in the tools to temper the
shortage of human experts.

\preston{Discuss what properties an automated testing tool must have}

The second point is that one can not simply grab any tool off the shelf (or
internet).  Any selected tool must capable of testing the system in
question, be usable by the developers on staff, and produce
high-quality results.  Related to this is a fourth consideration: in
addition to a general shortage of development capacity, one must consider
each developer's skill sets and expertise will vary.
In order to be successful, a
testing tool must be usable by all developers working on a system. If
lack of the necessary background means a developer can not 
deal with a tool's output,
then any effort in learning how to implement the tool or  evaluate its
results will be wasted.

The fourth and final variable is the quality of the tool's output.  Tools
that produce a high number of false positives are quickly deemed
untrustworthy.  A study by ZZZZ has shown that false positive rates as low
as Q\% have a dramatic, negative impact on developer perceptions of a tool.
\preston{cite}  A similar finding is supported by current psychological
literature.  Individuals tend to over-emphasize and over-predict
statistically rare negative events.\preston{cite}  Put another way, if a
tool leads to a developer wasting his or her afternoon by reporting a bug
that doesn't exist, the developer is going to mistrust that tool from then
on out.

% A useful tool must also do a good job of localizing the source of a bug.
% Tools that fail to identify (or worse, misidentify) the source of bugs with
% sufficient detail can also lead to wasted time and effort.  For example, a
% fuzzing tool that only interacts with an application by mutating normal
% input may eventually uncover a crash but merely reporting the presence of a
% crash-causing input doesn't get the developer very close to identifying and
% fixing the responsible block of code.
%
% \preston{This argument feels weak to me}
% At the same time, output that is too detailed or low level can prove just
% as detrimental to the bug fixing process.  Consider the case where a tool
% reports a bug takes place during the execution of code inside a library
% compiled without debugging symbols.  If the developer addressing the bug is
% unfamiliar with working on disassembled code, identifying the cause of the
% bug may be difficult.

\preston{not sure how to word this given that this might be the second
paper}

In this paper we evaluate CrashSimulator, a tool that tests applications by
exposing them to unusual environmental conditions 
that could be found in a deployment environment.  The conditions are
encoded as modifications to the results and side effects of system calls
made by the tested application.
Our goal in this is to show that CrashSimulator meets the above
criteria and compares favorably against similar tools, particularly in
terms of its usability.  CrashSimulator's
built-in set of anomalous conditions allows its users to define which
conditions to test an application against, thus alleviating the need to
have an expert knowledgeable enough to set up and test an application in a
chosen environment.  Because it also allows
application misbehavior to be localized
to a sequence of system calls the true source of a bug can be more
quickly identified,
and because of its low false positive rate and it
is a tool that does not waste developer's precious time.
Best of all, CrashSimulator allows developers of varying skill levels to
identify bugs in real world applications with a high degree of confidence.

\preston{Talk more about what sorts of questions we asked for the qualitative
answers.}
To validate these claims we conducted a study with ZZZ participants
consisting of Master's computer science students with varying backgrounds
and specializations.  We asked these participants to test existing popular
applications (as ranked by Debian's Popularity Contest) using each of the
tools.  From this work we collected both quantitative results in terms of
numbers of bugs identified, and qualitative results about the tools' user
experience through surveys.

The main contributions in this work are as follows:
\preston{We probably need another contribution that talks about a qualitative
concern.  How many people quit?
We can bring in my own observations of how easy the tool was to work
with.}
\begin{enumerate}

\item We illustrate that CrashSimulator's powerful system call manipulation
    capabilities and existing corpus of environmental anomalies makes it
        useful to developers with varying degrees of expertise.

\item We show that CrashSimulator allows these developers to find real bugs
in real applications.

\item We demonstrate that CrashSimulator compares favorably against similar
automated testing tools in the number of bugs found.

\end{enumerate}
