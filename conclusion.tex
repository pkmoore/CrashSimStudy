\section{Conclusion}
\label{SEC:conclusion}

As our first round of tests proved,
applications often fail upon deployment because of unexpected interactions
with its environment.
Although finding and eliminating
faults in applications is a key concern for software developers, it is
impractical to do actual tests in every possible
environment it may face.
To address this problem, we developed the SEA
technique that can be used to determine whether an application will
respond correctly to unexpected conditions.

We built a concrete implementation of SEA
called CrashSimulator which implements
the technique by simulating environmental
anomalies visible in the system calls an application makes.
Operating on system calls gives the tool a ``universal'' way to
encode and inject anomalies. Consequently, a set of mutations can be
collected from existing applications for use in testing others.
In this way, an ever-expanding corpus of anomalies can be
created, allowing lessons learned from bugs in one application to benefit
many others.
Our evaluation of CrashSimulator
has shown that this technique works and is
effective at finding bugs in well tested software.
In total,
XXXX new bugs were identified in popular applications.
Of these, YYYY been reported to the
affected parties.

Given that the technique as proven sound
and our has proven effective we will continue to improve upon it.
We envision a public repository of anomalies
that can be applied to new or existing applications.
We are also exploring
opportunities to further automate the discovery process
and improve the way anomalies are specified using a
domain specific language.
Further future work
will focus on analyzing how an
application attempts
to recover from the anomalies.  This would allow
us to determine whether
an application is correctly recovering
from an error, or carrying out some incorrect response.
