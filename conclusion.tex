\section{Conclusion}
\label{SEC:conclusion}

As our first round of tests proved, an unexpected interaction between an
application and its environment can cause
an application to fail upon deployment.
Although finding and eliminating
faults in applications is a key concern for software developers, it is
impractical to do actual tests of an application in every possible
environment it may face.
To address this problem, we developed and tested
CrashSimulator, a strategy and tool
that can determine whether an application will
respond correctly to anomalous environmental conditions.

After testing CrashSimulator with a number of
applications, we found that our technique of
operating on system calls gives it a ``universal'' way to
encode and inject anomalies. Consequently, a set of mutations can be
collected from any existing application for use in testing other new or
existing applications. In this way, an ever-expanding ``test suite'' can be
created, allowing lessons learned from bugs in one application to benefit
many others.

In addition, our evaluation of CrashSimulator has shown it to be both usable and
effective.  In our survey of developers, it compared favorably
against both AFL and Mutiny across self-reported developer skill levels.
CrashSimulator was particularly favored by developers with more operating
systems experience, but even developers with low OS experience were pleased
with its ability to let them find bugs by taking advantage of
the expert knowledge encoded in the tool.
In our user study, a total of
19 new bugs were identified in popular applications.
These bugs have been reported to the
affected parties and 5 have already been corrected.

%  CrashSimulator was able to identify bugs related to unusual
%  file types in 15 applications and bugs related to slow network performance
%  in 10 network applications and libraries.  Additionally CrashSimulator
%  found filesystem related bugs in 6 applications and libraries with
%  facilities for moving files within a system's filesystem.  The low overhead
%  introduced by CrashSimulator's technique meant that it was able to find
%  these bugs quickly in spite of its unoptimized implementation.

In the long term, we
envision a public repository of anomalies along with CrashSimulator test
patterns that can be applied to new or existing applications.
We are also exploring
opportunities to further automate the discovery process.  Beyond these logical
evolutions of the tool our research will proceed in
two different directions.  First, we plan on improving
its bug detection capabilities by analyzing how an
application attempts
to recover from the anomalies to which they are exposed.  This would allow
us to make determinations about whether an application is correctly recovering
from an error rather than indicating only the presence of some sort of response.
Second, we will work on making it possible for CrashSimulator to
understand and mutate common data formats, allowing it
to fuzz test data processing code while taking advantage of its
replay-based architecture to avoid re-executing portions of the application
that are not involved in the test.
