\section{Defining and Applying the Results Based Simulation Technique}
\label{SEC:approach}

\begin{figure}[t]
  \center{}
  \fbox{\includegraphics[scale=.37]{images/approach}}
  \caption{Diagram illustrating CrashSimulator's approach}
  \label{figure:approach}
\end{figure}

\begin{figure*}[t]
  \center{}
  \fbox{\includegraphics[scale=.65]{images/architecture}}
  \caption{Diagram illustrating CrashSimulator's Architecture.  During the
    course of a single rr execution, clone process sets are generated at
    specific rr events.  A CrashSimlator supervisor process attaches to
    these process sets and uses a strace-style system call listing to feed
    subsequent system call activity and inject unusual environmental
    conditions.}
  \label{figure:architecture}
\end{figure*}

As mentioned earlier,
the Results Based Simulation (RBS) technique
offers a reliable way to identify bugs
that could arise from interaction with a given environment.
In this section,
we will break down the elements of the technique,
and then introduce a tool called CrashSimulator,
which we built as a concrete implementation of RBS.
We also describe a second technology
called Process Set Cloning
that evolved during our work on CrashSimulator.


\subsection{Stepping through RBS}

The first step in implementing RBS
is selecting an anomaly or anomalies
against which you wish to test applications.
Anomalies can be collected
by examining failures of other applications
in a target deployment environment,
exploring public bug trackers,
or by using tools that can identify
potentially problematic environmental conditions~\cite{Zhuang_NSDI_2014,
rasley2015detecting}.
Identifying anomalies through bugs posted on public trackers
is ideal when determining
whether or not an application
is vulnerable to a widely publicized bug.
The chosen anomalies are then examined
in order to determine how they cause an application's communications
with its environment
to differ from a normal execution.
Once they have been teased out,
these differences can be represented
as a set of modifications
that must be made to an application's communications
in order to simulate the chosen anomaly or anomalies.
As an example,
consider an anomalous environment
where access to a required file is denied because of
the environment's configured file permissions.
This anomaly presents as attempts to access the file,
such as calls to {\tt fread()},
the {\tt read()} system call,
or other file access mechanisms,
returning an error stating that access to the file is denied.
The ``modification'' required to simulate this anomaly
is to change the results of similar accesses
in a test execution
to return ``access denied.''

The second step
is identifying
a way to monitor an application's communication
with its environment,
by redirecting function calls,
observing memory access,
monitoring network activity,
intercepting system calls,
or some other similar means.
These opportunities are identified by looking at the parameters,
return values,
and orderings,
of an application's communications
that indicate
it is an appropriate time to simulate an anomaly.
Monitoring an application's communications
in this fashion
has the added benefit
of allowing us to find obvious situations
where an application will fail
to handle certain environmental features.
This is possible because an application's failure
to carry out required operations is visible as it interacts
with its environment.
Consider an application that fails to check whether or not
a file already exists
before destructively opening it and writing to it.
The absence of this check
is problematic in environments where the file already exists
as it will be blindly overwritten.
Because it is obvious from the sequence of communications
the application makes when writing to the file
that this check has been neglected,
no simulation is required to identify the presence of a bug.

The third step in RBS
involves interposing on communications
and making the modifications necessary
to simulate the presence
of an environmental anomaly.
This could be accomplished by
influencing the results of function calls,
strategically altering memory values,
altering the results of system calls,
or some other method
of presenting modified communications to an application.
In the simplest case,
simulating an anomaly only requires
the modification of a single value.
In more complex cases,
large numbers of diverse communications
need to be interdicted and altered
in order to correctly simulate an anomaly.
For example,
simulating an erratic system clock
requires that all efforts
to access the clock
be correctly modified
to reflect the chosen aberration.

Lastly, the fourth RBS step
is to analyze an application's response
to a simulated anomaly.
This is accomplished by evaluating an application's
communications after it has been exposed
to a simulated anomaly.
The simplest conclusion to draw
is whether or not the application
has made an effort to respond
to the anomaly.
This determination can be made based
on the assumption that the way an
application deals with the anomaly will yield
different program paths (and therefore different communications) than
it would execute absent the anomaly.
If the application
does not alter its behavior, it has not
correctly handled this flaw --- a failing result.
Alternatively,
if the application does deviate,
it is likely that the application is taking some
action to handle the injected condition --- an indication of possibly
correct behavior.
This approach is often sufficient
to classify application behavior.
As with traditional tests,
CrashSimulator does not tell us that an application
is bug free in a given situation;
instead,
it asserts that an application
has incorrectly handled a given situation.

A limitation of the above strategy
is that it can result in false negatives
when application's change their behavior
but do not handle the anomaly correctly.
These false negatives can be addressed
by more detailed analysis
of the application's post simulation behavior.
This more detailed analysis
depends upon knowing what a correct response
to an anomaly looks like.
This ``known good'' behavior can be found
in a number of ways
including by looking at standards and documentation
that describe best practices for handling an anomaly
in a given environment
and examining how applications that correctly
deal with the anomaly do so.
Consider the case where the system call {\tt close()} fails.
Whether or not retrying the call is correct
depends upon what environment is being simulated.
Instead, the application's post-simulation communications
must be examined in enough detail to identify a repeated {\tt close()}
call taking into account
what is correct for the environment being simulated.


\subsection{CrashSimulator: A Concrete RBS Implementation}
