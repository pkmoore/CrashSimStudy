\section{How CrashSimulator Works}
\label{SEC:approach}

\begin{figure}[t]
  \center{}
  \fbox{\includegraphics[scale=.5]{approach}}
  \caption{Diagram illustrating CrashSimulator's approach}
  \label{figure:approach}
\end{figure}

CrashSimulator uses a four-step operation to identify environmental bugs.
These steps, illustrated in Figure~\ref{figure:approach}, include:
gathering the required anomaly information, transforming that information
into mutated traces, and interpreting  the output its replay and analysis
process yields.


\subsection{Anomaly Identification} \label{subsec:anomalyidentification}

The first step in CrashSimulator's testing process is identifying an
environmental anomaly against which to test an application.  This can be
accomplished either manually or by using existing tools to examine results
of other applications
running in the target environment.  For example, this
work utilized the results of both NetCheck~\cite{Zhuang_NSDI_2014} and
CheckAPI~\cite{rasley2015detecting}, along with some
manual analysis, to develop
the set of anomalies used in our evaluation.  NetCheck, which can be
thought of as a precursor to CrashSimulator, is a tool that can determine
the cause of a failure in a networked application, without any specific
knowledge of the application or its language. It is readily able to
identify a wide variety of anomalous behaviors related to network
communication between one or more hosts.  CheckAPI performs a similar
function by comparing the behavior of an API implementation to a reference
model.

Anomalies can be located and isolated via a number of methods. One source
are the public bug trackers of projects available on the web. If the
results and side effects of a series of system calls indicate the presence
of a bug,  CrashSimulator can inject these same results and side effects
at appropriate times in other applications.
This identifying anomalies in this fashion is ideal when determining
whether or not an application is vulnerable to a widely publicized bug. In
additional projectâ€™s set of available anomalies can be supplemented by bugs
taken from similar projects.

Anomalies can also be found around more complex operations.  Moving files
around in a filesystem is an extremely common operation that, on face
value, seems fairly simple to carry out.  In reality, things become
complex quickly once a few implementation details are taken into account:
namely limitations in Linux's {\tt rename()} system call and the way
Linux's directory hierarchy can abstract away complicated device
configurations.

\preston{This example might need to be changed.  One of the pieces of
feedback we received from reviewers was that some of our bugs could be
detected without replaying and injecting.  That is, they could be
identified by analyzing a dead strace recording from an application.  This
example will be much stronger if we inject rename() = -1 EXDEV and then
look for this pattern in the application's response once we have that
machinery in place.}

Trivial cases of moving a file (i.e. those where the source and destination
are on the same device) are accomplished using the {\tt rename()} system
call.  In cases where the source and destination are not on the same device
this system call fails and the application must handle the operation
manually. This means that \emph{any} application that moves files around on
a Linux system must be able to deal with a situation in which some part of
the directory structure may physically reside on a different device.
During our evaluation, we modeled what it takes to move a file correctly
using the GNU Coreutils {\tt mv} command as our ``gold standard.''  We
found many applications do not handle the above situation correctly
including the {\tt shutils} module from the Python 2.7.9 standard library.
It did not correctly preserve timestamps across the move, it did not
correctly apply extended file attributes to the destination file after the
move, and it did not verify the file's inode number between initially
examining the source file and when the copy was initiated (a common cause
of race conditions).  Failing to perform these operations exposes
applications relying on {\tt shutils} to misordered processing, data loss,
or security issues.

\subsection{Converting Anomalies to System Call Sequences}

%  This paragraph should cover the "describe manual effort" feedback
Once a new anomaly has been identified, it is distilled into a sequence of
system calls.  This process involves examining
system call traces and finding a series of calls indicative of
an inappropriate response to a specified anomaly.  This analysis should
identify any mutations that may need to be made to the trace
to simulate the anomalous environment (i.e. what modifications must be made
to system call return values and program state during replay).  Much like
the effort involved in constructing a unit test to check for correct
behavior in a piece of code, this process involves an initial outlay of
user skill and effort that will be paid back, as it can be
used repeatedly over time to test many different applications.

As a more concrete example of the above, consider an anomaly
involving unusual file types we will later address in our evaluation of
CrashSimulator.  This anomaly appears when a call to {\tt stat()} or a similar system
call returns a structure with a {\tt st\_mode} member containing an unexpected
value. Consider the line of {\tt strace} output representing a call to {\tt
  fstat()}:
\begin{quote}
  {\tt 8936  fstat64(3, \{st\_dev=makedev(0, 40), st\_ino=54993216, st\_mode=S\_IFREG ...\}) = 0}
\end{quote}
The third member of the returned structure indicates that the file is a
regular file by showing that {\tt st\_mode} flag is set in the {\tt st\_mode}
member.  CrashSimulator can mutate this  line to the following:

\begin{quote}
  {\tt 8936  fstat64(3, \{st\_dev=makedev(0, 40), st\_ino=54993216, st\_mode=S\_IFCHR ...\}) = 0}
\end{quote}

The trace containing this modified line can then be replayed in order to
test how the application responds when a file that is expected to be
regular is actually a character device. In our evaluation, we further
explore the behavior of applications as they encounter other possible valid
values of {\tt st\_mode} in the course of testing.


\subsection{Trace Recording and Anomaly Injection}

In the next step, a trace is generated by recording the application to be
tested as it executes in an environment where the chosen anomaly is not
present.
Once this trace has been generated, CrashSimulator's replay-based approach
can inject anomalies into an execution by introducing
the system call results and side effects
associated with the anomaly into the trace.
Because replaying the modified
trace faithfully follows the described system call behavior, the
application is exposed to the injected
anomalous conditions.  For
example, suppose an anomaly causes a call to {\tt read()} to fail with
return value of -1 and {\tt errno} set to {\tt EINVAL}.  Exposing an
application to this situation only requires editing the system call trace,
such that the line describing this read call reflects the above values.

Our implementation of CrashSimulator uses a combination of {\tt rr}, a
record-and-replay debugger, and our own process supervisor
to accomplish the above.  We rely on {\tt rr}
to store the information necessary to perform the bulk of uninteresting
replay that must take place before the application reaches the point in
execution where our tests will be performed.  At this point our own process
supervisor takes over replay responsibilities.  This phase of replay is
driven by a listing of
system call sequences written in the style of {\tt strace}
{\tt Strace} is a good fit
for this purpose because it provides a familiar way to describe an
application's system call activity in terms of a user-friendly
representation of inputs, outputs, and side effects.
This means CrashSimulator's user can easily modify
a {\tt strace} recording using a text editor to express
system call results and side effects associated with the anomaly.  These
modifications can typically be automated with the help of a "mutator"
script (and were for the anomalies used in our evaluation).

\subsection{Analyzing an Execution}

CrashSimulator's determination about whether an application may be
responding to an anomaly comes down to whether or not
the application alters its
system call behavior.  When an anomalous
behavior is injected into a replay execution,
it is expected that the application under
test will deal with it in some way.  For example, applications can
be expected to behave differently when the structure returned by a call to
{\tt stat()} indicates the target file is a symlink, rather than a regular
file.  CrashSimulator bases its assessment on the assumption that the way
an
application deals with the anomaly will yield
different program paths (and therefore different system calls) than
those
executed when the original system call trace was recorded.
If the application
does not alter its behavior, it has not
correctly handled this flaw --- a failing result.  Alternatively, if the
application does deviate, it is likely that the application is taking some
action to handle the injected condition --- an indication of possibly
correct behavior.

This approach is often sufficient to classify application behavior.  As
with traditional tests, CrashSimulator does not tell us that an application
is bug free in a given situation; instead, it asserts that an application
has incorrectly handled a given situation.  In our experience, actually
exposing applications to anomalies CrashSimulator reported as unhandled
did result
in bugs ranging from simple hangs to more dramatic crashes that
had the side effect of consuming all available resources or filling
available disk space with garbage.

It is important to understand that anomalies can
be as subtle as a single configuration flag set differently under
specific circumstances.  For example, in BSD and OS X, sockets returned by
the {\tt accept()} system call inherit the values of the {\tt O\_NONBLOCK}
and {\tt O\_ASYNC} flags from their parent socket.  In Linux, these values
are not inherited.  As a result, the correct behavior is to always
explicitly set these flags to the appropriate values on child sockets.
This environmental difference presented itself as a bug in the socket
implementation of Python 3.2.  Python took advantage of non-blocking
sockets in order to implement timeout capabilities for sockets.  Because
Python did not specifically set the {\tt O\_NONBLOCK} flag on the child
sockets underlying its socket object abstraction, it was possible that,
depending on the environment, the Python socket object would report the
socket as either blocking or non-blocking while the actual underlying
socket was configured for the opposite behavior.

\subsection{Building and Testing CrashSimulator}
\label{SEC:architecture}

\begin{figure}[t]
  \center{}
  \fbox{\includegraphics[scale=.75]{architecture}}
  \caption{Diagram illustrating CrashSimulator's Architecture.  During the
    course of a single rr execution, clone process sets are generated at
    specific rr events.  A CrashSimlator supervisor process attaches to
    these process sets and uses a strace-style system call listing to feed
    subsequent system call activity and inject unusual environmental
    conditions.}
  \label{figure:architecture}
\end{figure}

CrashSimulator's testing process for any application
consists of launching it as a child process and
interceeding in its execution at appropriate times in order to simulate the
results and side effects of any system call the application might make.
Its testing approach requires two pieces of
functionality, starting with a way to maintain
repeatable execution of the application.  This ensures that
the system call sequence required by the anomaly being tested is reached.
The second element required for its test protocol is a
way to interpose on an execution in order to modify the results and side
effects of system calls to reflect the environmental
anomaly.  Our current iteration of
CrashSimulator uses a modified version of {\tt rr} to handle the first
requirement and a custom process supervisor to handle the
second.

The {\tt rr} debugger is an excellent candidate
for handling repeatable executions
because of it can
record and replay applications quickly and accurately out right out of the
box.  This makes the tool valuable for both human-in-the-loop and automated
continuous integration/continuous deployment testing scenarios.  Replay
executions occur as a sequence of ``events'' that map to application
events like system calls and signals.  We modified {\tt rr} so that it
could be made to run to a specific event corresponding to a specific system
call and then generate clones of the set of processes being replayed at that
point in time.  These cloned process sets exist entirely separately from
{\tt rr}, allowing the tool to continue through
its replay process to the next
designated event.  One {\tt rr} replay execution can spin
off as many process sets as may be required to perform the set of tests
configured by the user.

Process sets generated by {\tt rr} are created in a stopped state and
remain until they are attached to and utilized by a CrashSimulator
supervisor.  Each process set has its own supervisor process to inject
its configured environmental anomaly.  The
supervisor wakes up the process set it is managing and simulates any
subsequent system calls it makes.  The data necessary for this
simulation is
supplied as a system call listing formatted after the style of {\tt strace}
output, that describes the results and side effects for each system
call. The output is engineered in such a way to contain the
elements required to reflect the
desired environmental anomaly.  Supervisors can complete this
process independently of one another, which lends a
high degree of speed and
parallelism to the whole CrashSimulator testing process.

To be able to evaluate how CrashSimulator performs, a series of tests were
conducted using a prototype build on {\tt rr} version 5.2.0 running on a
32-bit Linux kernel running distributed with  Ubuntu 16.04 LTS.  Our
modifications to {\tt rr} were carried out in C++ and the CrashSimulator
supervisor was implemented in ZZZZ lines of Python 2.7 code with a YYYY
line C extension that allows it to interact with processes using the {\tt
Ptrace} API.  This version of CrashSimulator is available as a Docker
container and, due to some operating system configuration being necessary,
is most easily installed in this fashion.

With the prototype in hand, we were ready to test its bug finding
abilities.  For a test environment, we chose
a vmware virtual machine running version of CrashSimulator described above.
We ran the tests on a 1.7 gHz
core i7 system with 8GB of RAM. Our implementation used {\tt ptrace} to
interpose on the running program and interject anomalous behavior into its
execution.  To our knowledge, the only major negative impact of
these design choices was that the use of {\tt ptrace} and Python did slow
down our
prototype's performance.  On the positive side, it simplified construction.
As will be discussed later, much of this performance
degradation is offset by the highly asynchronous fashion in which our
prototype can execute tests.

This prototype was evaluated in two phases.  The first phase set out to
prove that CrashSimulator's technique could work in the real world with a
reasonable degree of efficiency.  In this phase, the tool's developers used
it to find bugs in popular open source applications.  In the second phase,
we aimed to ascertain whether or not the tool could be used by developers
at various skill and experience levels, and to determine how its
capabilities stacked up against other well known automated testing tools.
