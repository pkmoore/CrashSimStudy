\section{Evaluation}
\label{SEC:evaluation}

CrashSimulator was designed
as a way to reduce the considerable effort
required of developers
in implementing and maintaining applications.
Therefore,
we needed to prove its effectiveness in ``the wild.''
To this end,
we carried out two rounds of
evaluation for CrashSimulator.
We started by exposing
a series of real-world applications
to a library of collected anomalies in a laboratory environment.
These tests,
conducted by the research team,
were followed by a user study
in which undergraduate and graduate computer science students
got a chance to use CrashSimulator
to identify new environmental bugs
in tests on applications of their choosing.
We used the results from both these efforts
to answer the following questions:

\begin{enumerate}

\item{Is CrashSimulator able to identify a wide variety of environmental
    bugs?
(Subsection~\ref{sec-env-bugs})}

\item{What Sorts of Errors Does CrashSimulator Make?
    (Subsection~\ref{sec-sorts-errors})}

\item{Can CrashSimulator
      execute tests efficiently? (Subsection~\ref{sec-perf})}

\end{enumerate}

\subsection{Is CrashSimulator able to identify a wide variety of
environmental bugs?}
\label{sec-env-bugs}

The most crucial question to ask about CrashSimulator
is how readily it can
identify bugs
caused by a wide variety of environmental differences.
We have already established
that these bugs can arise from
unusual filesystem and network situations,
so we configured the tool to look
for bugs that could be triggered by these features.
These bugs rely on common system calls,
and thus had the potential to
appear in many programs.
As we wanted to test
the most commonly used applications,
our selections were made
from among those deemed ``popular''
by Debian's Popularity Contest~\cite{DebPopCon},
or those used
by many Linux distributions,
such as ones provided
by the GNU Coreutils project.

\subsubsection{The simplest case - A Filesystem Bug Found With the Null Mutator}

In our first test we decided to evaluate the tool in its simplest possible
configuration -- employing the {\bf ``null mutator''}.
This mutator takes no action and simulates no anomalous conditions.
It simply allows checkers
a chance to evaluate an application's behavior
as it carries out a potentially-buggy operation unhindered.
To test CrashSimulator's ability to find bugs
in this fashion, we decided to look at how applications
move files about on the filesystem.
In many cases this operation can be handled
atomically by the operating system
through the {\tt rename()} system call.
However,
in situations where the source file and destination file
are on different storage devices,
the application must
manually perform all of the required steps.
This is a process
that even well-tested applications
frequently get wrong~\cite{PHPRenameBug,PythonShutilBug,NodejsCopyBug}.

\paragraph{Method.}  For this portion of our evaluation
CrashSimulator was configured
to test each of the applications listed in Table~\ref{table:crossdevice}
using the Null Mutator and a set of checkers constructed find applications
that fail to correctly move a file from one disk to another.
Catching this particular class of bugs requires
checkers that encode the correct steps involved in moving a file from one
storage device to another.
After examining several libraries and applications,
we found that
{\tt mv} seemed to handle cases that other tools failed to consider.
Therefore, we
used its behavior as a template to create a set of checkers
that evaluate whether or not
the application correctly performs the following
types of checks and operations:

{\bf Source Replaced.}

An application should make an effort
to ensure that the file being copied
is not replaced between the time it is initially examined
and the time it is opened for copying.
Otherwise,
it could have been replaced by a file of a different type,
such as a character device.
To test whether applications are robust
to anomalies involving source file replacement,
we developed a checker that monitors whether a series of safety checks
are performed.
If these checks are not performed it
means the application will proceed with its operations,
leaving open the possibility of file corruption~\cite{PythonShutilBug}.

{\bf Preserve Xattrs}

Extended file attributes
are used by modern operating systems
to store descriptive information
that the strictly defined and limited structure
of normal filesystem fields cannot hold.
For example,
an operating system can use extended file attributes
to record whether or not a file was downloaded from the Internet.
This is important security information
that is used to warn users
if they are about to open a potentially unsafe file.
Apple's Gatekeeper relies on extended file attributes
to prevent the execution of applications downloaded from
untrusted developers without explicit user action~\cite{AppleCodeSigning}.
When copying a file,
an application should retrieve extended file attributes from the source
file and, later, apply them to the destination file.
In this case, CrashSimulator used a checker
that watches for the application to make system calls
that read the extended file attributes from the source file (i.e. {\tt
  getxattr()}, {\tt lgetxattr()}, or {\tt fgetxattr()}) followed by system calls
that re-apply the attributes to the destination file (i.e. {\tt setxattr()},
{\tt lsetxattr()}, or {\tt fsetxattr()}).

{\bf Preserve Timestamps}
Incorrect timestamps can impede applications like
in {\tt make}, archival programs, and similar
software~\cite{NautilusTimestamps, SudoTimestamp}.
As a result, it is important to ensure
that time related metadata --
such as creation, modification, and access times
are preserved when copying a file.
To evaluate failures on this front,
we used a checker that determines
whether the application applies
the appropriate timestamps to the destination file
by monitoring for system calls from the {\tt stat()}
and {\tt utime()} families used to retrieve and apply timestamps.

{\bf Copying Devices}
It is also important to check if a move
would attempt to copy a special
device file, such as a named pipe, across disks.
Files of this variety must be moved
by creating a new device of the same type at the destination,
instead of exhaustively reading and writing its contents.
In our experience, applications that fail to perform this check
can end up completely filling a disks, exhausting available memory,
or blocking forever, which can cause the system to become unresponsive.


 \begin{table}[t]
    \scriptsize{}
    \begin{tabular}{l p{1cm} p{1cm} p{1.2cm} p{1cm}}
    \toprule{}
        Application     & Source Replaced & Preserve Xattrs & Preserve Timestamps & Copying Devices\\
\hline
        {\tt mv}              & Correct             & Correct         & Correct             & Correct\\
        {\tt mmv}             & Correct             & {\bf Sec. Flaw} & {\bf Time Loss} & Correct\\
        {\tt install}         & Correct             & {\bf Sec. Flaw} & {\bf Time Loss} & {\bf Fill Disk} \\
        {\tt perl File::Copy} & Correct             & {\bf Sec. Flaw} & {\bf Time Loss} & {\bf Fill Disk} \\
        {\tt shutils}         & {\bf Corrupt}	& {\bf Sec. Flaw} 	& Correct             & Correct\\
        {\tt rust}             & Correct             & {\bf Sec. Flaw} & {\bf Time Loss} & {\bf Fill Disk} \\
        {\tt boost::copyfile} & {\bf Corrupt}	      & {\bf Sec. Flaw} & {\bf Time Loss} & {\bf Fill Disk} \\
    \bottomrule{}
    \end{tabular}
    \caption{Applications and libraries analyzed to determine whether or not
      they are able to correctly move a file from one device to another.
Incorrect entries are either missing the needed check or were ineffective.}
    \label{table:crossdevice}
\end{table}

\paragraph{Findings.}
As can be seen from the results in Table~\ref{table:crossdevice}, each of the
applications tested fails to perform one or more of the steps required to
successfully complete a cross-device move.  This is an unfortunate situation
because a failure to perform any one of these steps can result in negative
outcomes for the system as a whole.
Our results indicate that CrashSimulator is able to identify whether complex
operations are performed correctly in anomalous situations in
an array of popular programs.
This includes the standard libraries for the programming languages Python,
Perl, and Rust, along with other widely used software.  This is true even
when many libraries have correct behavior in some cases, but not others.


\subsubsection{A More Complex Case - The Unexpected File Types Mutator}
\label{sec-file-type-bugs}

Employing a mutator
with more complexity than the null mutator
allows CrashSimulator to inject anomalies into an execution
of an application.
This introduces problematic scenarios
so that the way an application responds
can be evaluated.
To test
the tool's capabilities
at this point,
we needed an anomaly
that would arise during a common situation,
such as when a Linux application retrieves
and processes data from a file.
Linux supports
several special file types,
including
directories,
symbolic links,
character devices,
block devices,
sockets, and
First-In-First-Out (FIFO) pipes.
These special files
use the same system calls
as regular files
(such as {\tt read()} and {\tt write()}),
but they behave in very different ways.
For example,
{\tt /dev/urandom} is a character device
that produces an infinite amount
of pseudo-random data
when read.
If an application that reads the full
contents of a file before processing is provided {\tt /dev/urandom}, it
will fill memory or disk space and could
crash the system~\cite{YumAptEndless}.
Correct execution in these situations
requires applications
to examine the files so they do not
interact inappropriately with a given file type.

\paragraph{Method.}
In order to confirm the accuracy of CrashSimulator's assessments we
exposed a subset of the Coreutils applications tested to
a simulation of the unusual file
types to get an idea of how they would respond.
Identifying these bugs involves changing an application's
execution to induce its response to an unexpected file type.  For
example, the {\tt sed} application, which modifies the contents of a text
file according to a provided command string, could be provided a symbolic
link, a directory, or a character device instead.  CrashSimulator
accomplishes this by identifying the calls to {\tt stat()}, {\tt fstat()},
or {\tt lstat()} that an application makes to examine the file, and then
changing their results to simulate
one of the special file types.  If the application responds to
this injected information then there is the possibility that the special
file will be handled correctly.  On the other hand, if there is no
alteration in the behavior of the application,  then the condition is not
being handled correctly.

For each application,
CrashSimulator was configured to simulate all of the non-standard file
types and
Table~\ref{table:unexpectedtypes} shows the values that CrashSimulator
inserted into the results of the {\tt stat()}-like calls made by the
application.
A result of ``Expected Type (ET)'' indicates
that this is the file type the application was provided
when the application was initially recorded -- that is,
a file of the type the application was expecting.
A result of ``\tickmark'' indicates that the application
identified it was being provided with an unexpected file type and its
execution diverged,
indicating that it was potentially handling the
unexpected file type correctly.
A result of ``\xmark'' indicates that the
application failed to recognize the presence of the unusual file type
because execution never diverged from the trace being replayed.
We manually examined a subset of the listed application behaviors simulated by
CrashSimulator and verified they were consistent with the actual behavior
on the given file type.

\paragraph{Findings.}
The frequency of failed executions in our results,
which are shown in Table 2,
indicates that many
applications make the assumption that they will only be used to process
regular files.  When this assumption does not hold, execution results
can be hard to predict.
In many cases a denial of
service condition occurs in the form of the application ``hanging,'' as it
attempts to incorrectly process the file.
This is typically the result of
an application blocking forever as it waits for a {\tt read()}
call to retrieve non-existent data from an empty FIFO,
or an application attempting
to read in and process an
``infinitely large'' file.
This situation is particularly dangerous as
it can eventually lead to
fill all available memory or disk space being filled~\cite{Cappos_CCS_08}.


\begin{table*}[t]
    \scriptsize{}
    \begin{tabular}{l  l  |  l  l  l  l  l  l  l}
    \toprule{}
        Application       & Condition Tested           & Regular File & Directory & Character Device & Block Device & Named Pipe & Symbolic Link & Socket File (IFSOCK)\\
                          &                            &  (IFREG)     & (IFDIR)   & (IFCHR)          & (IFBLK)      & (IFIFO)    & (IFLNK)       & (IFSOCK)\\
\hline
        {\tt Aspell}      & Dictionary File            & ET        & \xmark     & \tickmark  & \xmark    & \xmark        & \xmark       & \xmark\\
        {\tt Aspell}      & File being checked         & ET        & \xmark     & \tickmark  & \xmark    & \xmark        & \xmark       & \xmark\\
        {\tt gnu-gpg}     & secring.gpg                & ET        & \xmark     & \xmark     & \xmark    & \xmark        & \xmark       & \xmark\\
        {\tt vim}         & File being opened          & ET        & \tickmark  & \tickmark  & \tickmark & \tickmark     & \tickmark    & \xmark\\
        {\tt nano}        & File being opened          & ET        & \tickmark  & \tickmark  & \tickmark & \xmark        & \xmark       & \xmark\\
        {\tt sed}         & File being edited          & ET        & \xmark     & \tickmark  & \xmark    & \xmark        & \xmark       & \xmark\\
        {\tt df}          & /proc                      & \xmark    & ET         & \xmark     & \xmark    & \xmark        & \xmark       & \xmark\\
        {\tt wc}          & File being checked         & ET        & \tickmark  & \tickmark  & \tickmark & \tickmark     & \tickmark    & \tickmark\\
        {\tt du}          & Directory being checked    & \tickmark & ET         & \tickmark  & \tickmark & \tickmark     & \tickmark    & \tickmark\\
        {\tt install}     & File being installed       & ET        & \tickmark  & \xmark     & \xmark    & \xmark        & \tickmark    & \xmark\\
        {\tt fmt}         & File being formatted       & ET        & \xmark     & \tickmark  & \xmark    & \xmark        & \xmark       & \xmark\\
        {\tt od}          & File being dumped          & ET        & \tickmark  & \tickmark  & \xmark    & \xmark        & \xmark       & \xmark\\
        {\tt ptx}         & File being read            & ET        & \tickmark  & \tickmark  & \tickmark & \tickmark     & \tickmark    & \tickmark\\
        {\tt comm}        & Second file being compared & ET        & \tickmark  & \tickmark  & \xmark    & \xmark        & \xmark       & \xmark\\
        {\tt pr}          & File being read            & ET        & \tickmark  & \xmark     & \xmark    & \xmark        & \xmark       & \xmark\\
\hline
        \multicolumn{9}{l}{\scriptsize{\tickmark  $=$ CrashSimulator
        predicts application will recognize anomaly}}\\
        \multicolumn{9}{l}{\scriptsize{\xmark  $=$ CrashSimulator predicts
        application will fail to recognize anomaly}}\\
        \multicolumn{9}{l}{\scriptsize{ET (Expected Type)  $=$ File type expected by the
        application}}\\
    \bottomrule{}
    \end{tabular}
    \caption{Applications tested for their handling of unexpected file types.  A
    result of ``\tickmark'' indicates that the application identified the
    presence of an unusual file and responded in some fashion.  A result of
    ``\xmark'' indicates that the application failed to recognize the presence of
    an unusual file and attempted to process it.}
    \label{table:unexpectedtypes}
\end{table*}


\begin{table*}[t]
    \scriptsize{}
    \begin{tabular}{l  l  l  l  l  l  l  l  l}
    \toprule{}
        Application         & Directory                 & Character Device & Block Device  & Named Pipe \\
        Application         & (IFDIR)                   & (IFCHR) & (IFBLK) & (FIFO) \\
\hline
        {\tt wc}            & Error: Is a Directory     & hangs       & slowly process file  & Hangs\\
        {\tt install}       & Error: Omitting Directory & Fills disk  & slowly copies file   & Hangs\\
        {\tt fmt}           & No output                 & hangs       & garbage output       & Hangs\\
        {\tt od}            & Error: read error         & hangs       & No output            & Hangs\\
        {\tt ptx}           & Error: Is a Directory     & fills disk  & garbage output       & Hangs\\
        {\tt comm}          & Error: Is a Directory     & hangs       & garbage output       & Hangs\\
        {\tt pr}            & Error: Is a Directory     & hangs       & garbage output       & Hangs\\
\hline
    \bottomrule{}
    \end{tabular}
    \caption{Responses of a sample of coreutils applications when exposed to
      anomalous conditions.  The character device used was the infinite-length {\tt
        /dev/urandom}.}
    \label{table:applicationresponses}
\end{table*}


Table~\ref{table:applicationresponses} contains the results of this test.
CrashSimulator's evaluation of an application can map to real world bug behavior
in a few different combinations
One
possibility is that CrashSimulator asserts that the application will fail
and in practice, it does.  This is the case when we evaluated
the response of  {\tt install} being provided a character device
rather than a regular file. CrashSimulator predicted failure and the
application ended up filling the disk of the machine on which it was run.  The
opposite can also occur.  That is, CrashSimulator reports that the
application detected the anomalous condition and the application manages to
do so in practice,  as when we evaluated {\tt wc}'s successful response to
being run on a directory.

\subsubsection{Beyond Filesystem Bugs - Poorly Configured Network Timeouts}
\label{sec-timeout-bugs}

CrashSimulator is not limited
to identifying filesystem-based anomalies.
The third anomaly we examined
involves an application's behavior
when it attempts to communicate
over a network with extremely long
(on the order of minutes) response times.
At a low level,
applications retrieve data from a network socket
by waiting for data to be available and then reading it.
However,
this approach needs to be able to handle
a situation where communication
takes too long and should time out.

\paragraph{Method.}
CrashSimulator can detect
whether an application is vulnerable to this attack
by employing the null mutator and a network timeout checker
in order to determine if it makes any effort
to configure its network communications with a timeout value.
This is done
by examining the presence or absence of {\tt setsockopt()}, {\tt poll()}
and {\tt select()} calls as well as the timeout values that may
have been passed to them. Applications that do not set the timeout are
subject to the operating system-defined protocol timeout value (19 minutes
on Linux).
CrashSimulator is able to take this analysis a step further by employing a
Long Network Response Time mutator
that manipulates
the results of all time-returning calls,
simulating an execution where close
to the maximum timeout value occurs,
without actually spending any time
waiting.

An application's failure
to time out responsibly
is not just an inconvenience --
it can be taken advantage of by attackers
to consume resources and potentially cause a denial of service situation.
This failure was exploited by the slowloris~\cite{Slowloris} tool
to enhance the ability
of a small number of computers to prevent access
to vulnerable web servers
by opening and maintaining connections to the servers
for extremely long periods of time.
As these servers could only handle a set number of connections
due to resource constraints,
legitimate traffic was easily crowded out by the attackers.
Additionally, similar attacks can be used to
indefinitely delay security updates to
clients, leaving them vulnerable to compromise~\cite{Cappos_TR_08}.
We used CrashSimulator to determine which applications and
libraries from a selection based on Debian's ratings~\cite{DebPopCon}
could be vulnerable to this sort of attack.

\begin{table}[t]
  \scriptsize{}
  \begin{tabular}{l | l}
    \toprule{}
    {\bf Application}              & {\bf Analysis Result}\\
    {\tt wget}                     & Overly long timeout supplied to {\tt select()} \\
    {\tt ftp}                      & No {\tt poll()} or {\tt select()}, no timeout set \\
    {\tt telnet}                   & {\tt select()} specifies no timeout \\
    {\tt urllib http}              & No {\tt poll()} or {\tt select()}, no timeout set \\
    {\tt urllib ftp}               & No {\tt poll()} or {\tt select()}, no timeout set \\
    {\tt ftplib}                   & No {\tt poll()} or {\tt select()}, no timeout set \\
    {\tt httplib}                  & No {\tt poll()} or {\tt select()}, no timeout set \\
    {\tt requests}                 & No {\tt poll()} or {\tt select()}, no timeout set \\
    {\tt urllib3}                  & No {\tt poll()} or {\tt select()}, no timeout set \\
    {\tt python-websocket-client}  & No {\tt poll()} or {\tt select()}, no timeout set \\
    \bottomrule{}
  \end{tabular}
  \caption{Applications tested for their handling of extremely slow response
    times from the host with which they are communicating }
  \label{table:slowloris}
\end{table}


\paragraph{Findings.}
As Table~\ref{table:slowloris} shows, all of these
applications were vulnerable to this sort of anomaly,
and in some cases,
the timeouts took hours to resolve.
What's more, in the vast majority of
cases, the problem occurs because the application makes no effort to
specify a timeout value.  This means an attacker can transmit one byte of
data per timeout period (per Linux's value of 19 minutes for TCP sockets),
allowing them to keep the application alive instead of quitting.

\subsubsection{Bugs Found By Participants}

Another topic of relevance
in evaluating CrashSimulator
is how well developers are able to use the tool
to find bugs.
To investigate this angle
we conducted a user study
with 12 undergraduate and graduate students.
These students had varying levels
of operating systems experience
and minimal training with the tool.
Still, study participants found a total of 11 bugs using CrashSimulator.
Of these bugs, nine were found using the ``Unusual Filetype'' mutator.
Five of these bugs have since been reported to the appropriate maintainers,
and three of these reports included patches built by the reporting student
that correct the bug.

These results are important
because they confirm
users other than the original development team
for CrashSimlator
can use it to find new bugs in real world applications.
Participants commented that narrowing the source of a bug
down to a particular sequence of system calls
was helpful in identifying the area of
code responsible for the bug -- a feature
that decreased the time required to produce a fix.
Though observation of study participants
showed that familiarity with operating systems concepts
made it easier to work with CrashSimulator,
those without this background were still able to identify bugs using the
built in anomalies.


On a less positive note,
the study did reveal
some shortcomings
of the tool.
First,
it revealed that the tool
does not have a clear mechanism
for determining
what application behaviors constitute a bug.
For example,
it may be the intention of an application's developer
to have the application run continuously
until killed by an outside command
when processing an ``infinitely long'' file,
and therefore that behavior should not be classified as a bug.
Second,
it demonstrated that
simply reporting an application did or did not change its behavior
in the presence of an anomaly is insufficient
if CrashSimulator's user is unfamiliar
with what each result indicates in terms of the presence of a bug.
Both of these issues are being corrected
by improving what the tool outputs.
By more clearly describing
the nature of a given result,
users can have a better idea
if,
and why,
they should be concerned.


\subsection{What Sorts of Errors does CrashSimulator Make?}
\label{sec-sorts-errors}

\textbf{False Positives.}
Like most other testing tools, CrashSimulator occasionally makes
false positive reports.
Because these errors waste developer time and effort,
being able to adjust CrashSimulator to eliminate them when they are
encountered is a key concern.
The primary source of false positives in CrashSimulator is an application
using a different sequence of system calls to implement a given operation
than was expected by mutator.
CrashSimulator's approach allows these
situations to be easily corrected, once identified.
This is similar to the circumstance where an
application's test suite is missing a test, necessitating action by the
developer to construct it.

For example, consider GNOME's {\tt glib} file handling functions.  When an
application makes use of these facilities to move a file across storage
devices the library itself correctly performs a
file move operation.  When we used CrashSimulator with
checkers that expected a calls to {\tt read()} and {\tt write()}
for a cross-device move, we got reports stating that the
application {\em did not} perform the system calls necessary to inject
an anomaly.  By manually
examining a system call trace, we found that, while {\tt glib} correctly
performs the requested move operation,
it does so using alternative system call
sequences.  Rather than using a sequence of {\tt read()} and {\tt write()}
calls, as our checker expected, {\tt glib} creates a pipe and uses the {\tt
splice()} system call to copy the contents out of the source file, through
the pipe and into the destination file.

Fortunately, as soon as issues like this are discovered,
CrashSimulator's checkers can be modified to include the alternative
approach.
Given the above example around moving
files, consider the mapping from high level ``operation'' to the set of
system calls that can implement it in Table~\ref{table:stepsandcalls}.
Each of the steps in the operation map to a small number of system calls
and,
in
situations where two system call sequences can correctly implement the same
operation, CrashSimulator simply runs two checkers in parallel resulting in a
test executing for the one whose prerequisites are met.

\begin{table}[t]
    \scriptsize{}
    \begin{tabular}{l | l }
    \toprule{}
      {\bf Operation}                                               & {\bf Potential System calls}\\
      Examine source file                                     & stat64(), lstat64(), fstat64()\\
      Examine destination file                                & stat64(), lstat64(), fstat64()\\
      Open source file                                        & open()\\
      Read contents of source file                            & read(), splice() with a pipe\\
      List source file's & \\ ~~~~~extended file attributes             & listxattr(), llistxattr(), flistxattr()\\
      %Read contents of source file's extended file attributes & getxattr(), lgetxattr(), fgetxattr()\\
      Read contents of source file's                    & \\
      ~~~~~~~extended file attributes & getxattr(), lgetxattr(), fgetxattr() \\
      Open destination file                                   & open(), optionally unlink() the file first\\
      Write contents to destination file                      & write(), splice() with a pipe\\
      Apply extended file attributes to & \\ ~~~~~destination file      & setxattr(), lsetxattr(), fsetxattr()\\
      Apply proper timestamps to & \\ ~~~~~destination file             & utimens(), futimens()\\
      Apply proper permissions & \\ ~~~~~to destination file            & chmod(), open() with a modeline specified\\
      Close the source file                                   & close()\\
      Close the destination file                              & close()\\
    \bottomrule{}
    \end{tabular}
    \caption{Each step of a successful cross-disk file move operation mapped to
      the system call or calls that can implement it}
    \label{table:stepsandcalls}
\end{table}

A second source of false positives is a test being configured such that not
enough of the application's execution is included in the test.
One implementation detail of CrashSimulator is that,
by default,
its tests evaluate an application's behavior
within a defined (but configurable) number of system calls.
We found in some cases
that an application's error handling or recovery code
may take place outside of this span of execution.
False positives
of this sort
are easily corrected
by expanding the length of execution
covered by the test.
The low number of occurrences of this
type of false positive gives us confidence that the default configuration
is satisfactory in the majority of cases.

\textbf{False Negatives.}

In some cases,
using the default checker resulted
in CrashSimulator making false negative reports.
This happens when
an application changes its behavior
but still does not handle an anomaly correctly.
These false negatives can be addressed
by performing a more detailed analysis
of the application's post simulation behavior.
Before testing,
a user must know
what a correct response
to an anomaly looks like.
This ``known good'' behavior can be found
by looking at standards and documentation
that describe best practices for handling an anomaly
in a given environment,
or by examining how applications that correctly
deal with the anomaly do so.
Consider the case where a {\tt close()} system call fails.
Retrying the call may not be the correct action,
depending upon the environment in question.
SEA can be used to determine if an application
has handled the failure correctly
by examining post-simulation communications in detail,
and taking into account the correctness of retrying the call.

\subsection{Can CrashSimulator execute tests efficiently?}
\label{sec-perf}

One key attribute of successful testing tools is that they are able to
complete their tests in a timely manner.  If a tool takes too long,
users will be less likely to run it.
To address this concern, the performance of CrashSimulator was
evaluated in order to determine whether or not it was able to complete its
test executions in an acceptable time frame.

\paragraph{Method.}
We examined the completion
times for executions of the specified application in both
native and under CrashSimulator configured to test using the ``Unusual File
Types'' anomaly discussed earlier.
Figure~\ref{figure:performance} shows these results.

    \begin{figure}[t]
        \center{}
        \fbox{\includegraphics[scale=.75]{images/performance.png}}
        \caption{\emph{This shows the run time difference between the
native program and CrashSimulator in seconds.  Each dot indicates an
        execution.  The X axis shows time values for native executions, and
        executions under CrashSimulator with 0, 5, 10, and 100 tests
        performed respectively.
}}
         \label{figure:performance}

    \end{figure}


\paragraph{Findings.}
Overall, the performance of CrashSimulator is around
an order of magnitude slower than the original program being executed
natively.  It should be noted that in most cases
this slowdown is somewhat mitigated by CrashSimulator's ability to process
tests asynchronously and in other situations it will be more efficient than
running the program natively since {\tt rr's} replay does not require
actual execution of most system calls.  This means that CrashSimulator
avoids the system call overheads, such as I/O.
Even without these improvements, however, CrashSimulator's runtime is more
than manageable when the value it provides is taken into account.  The
cumulative runtime required to execute the tests required to find the QQQ
bugs listed in Table~\ref{table:unexpectedtypes} is around HHHH minutes.
Given this success, the increased runtime is worth the wait.
