\section{Evaluation}
\label{SEC:evaluation}

 % Limitation -> Will users be able to identify what caused the problematic
% system call sequence -> Add this to evaluation*

Our goal with this work was to gain an undestanding of how CrashSimulator's
user exprience compared to other tools and techniques available to
developers...

Because CrashSimulator is able to to test a wide domain of operations an
appliation can peform we felt it necessary to compare it with two separte
tools, one that test based on files and one that tests based on network
activity.  Specifically, we used AFL and Mutiny...


\subsection{Method}

We gathered a group of YYYY participants from a computer science master's
program.  These students had varying background in relation to software
development and automated testing.  These students all received AAA hours
of education on each of these tools.  This education focused on the tools'
initial setup, configuration, usage, and how to interpret the tools'
results.  BBB of the students had used AFL in previous course work.  None
of the students had previous experience with Mutiny or CrashSimulator.

The 20 participants in this study received instruction on the configuration
and usage of the above tools as well as CrashSimulator.  We asked these
users to test popular real-world applications with the goal of identifying
new bugs and reporting them to the application's developers.  In performing
this task our participants had to correctly set up and configure each of
the tools, execute a testing process, evaluate the results, and produce
sufficient proof of validity for each bug that a convincing case for fixing
it can be made.  Each of these steps is an opportunity for gaining insight
into how CrashSimulator stacks up against other tools.

We used 3 techniques to capture the experience of our participants as they
used each of the tools.  For quantitative concerns we examined the time
required for participants to carry out defined tasks relevant to the
process of bug hunting.  To do this, we asked users to track the time
required to perform each step of the process of finding and reporting a
bug.


We also captured hard counts of bugs identified by our particpants using
each of the tools.  The specifics of these bugs will be discussed in more
detail......

For Qualititative concerns we gathered participant impressions on the tools
in use via surveys.  These surveys were strucured to give insight into how
CrashSimlator compares to the other tools in three areas:

\begin{itemize}

\item Usability of the tool. (i.e. set up, configuration, executing tests,
interpretring results)

\item Extending the tool through the creation of additional
anomalies and checkers

\item How well the tool is able to test the areas of an application
that they are interested in

\end{itemize}

Additionally, we asked our partipants to rate their own experience level in
software development, software testing, and operating systems concepts so
that we might determine whether or not an individual's background has an
effect on what tools they prefer.  Our hope was that this information would
provide insights into how a participants background affected their
expriences with the tools.

In this evaluation we set out to answer the following questions using both
the qualitiative and quantitative data collected during our study.

\begin{enumerate}

\item Are users able to effectively find bugs using CrashSimulator?  How
does CrashSimulator compare to other tools in this area?

\item How well does CrashSimulator allow users to test the areas of
applications they are interested in?  Are other tools able to better
evaluate these areas?

\item Does CrashSimulator allow its users to construct a test suite
efficiently?  How does CrashSimulator compare to similar tools
efficiency wise?

\item Does user experience level have an impact on tool experience?  Is
there a difference in the skillsets required in order to be effective
with CrashSimulator vs the other tools?

\item Does CrashSimulator provide output that is useful in locating and
fixing bugs?

\end{enumerate}


\subsection{Are users able to effectively find bugs using CrashSimulator?}

For any testing tool or technique the primary concern is whether or not it
is useful for finding bugs.  To evaluate this concern in CrashSimulator's
case we need to answer whether or not CrashSimulator's users are able to
effectively find bugs using the tool....


\subsubsection{Findings}

Table 1 contains counts of bugs identified broken down by the tool used to
identify them and self reported developer experience level with Operating
Systems concepts. A total of !!!!15!!!! bugs were found using
CrashSimulator with the majority coming from participants with a
self-reported high degree of operating systems experience.  Participants
with a moderate and low degrees of operating systems experience found more
bugs AFL.....

\begin{figure}[t]
  \center{}
  \includegraphics[scale=.5]{images/table1}
  \caption{\emph{Bugs Found by Tool and Experience Level}}
  \label{fig-tool-exp}
\end{figure}


\subsubsection{Discussion}

The above results indicate that, when it comes to using CrashSimulator,
having experience with operating systems concepts is very beneficial...
That said, users with a self-reported low degree of experience with OS
concepts were able to identify bugs using CrashSimulator's built in corpus
of anomalies and checkers...  Table 2 illustrates which of these checkers
were most effective......


\subsubsection{Discussion on Specific Bugs}

!!!! DISCUSSION !!!!


\subsection{How well does CrashSimulator allow users to test the areas of
applications they are interested in?}

Modern applications typically consist of many different parts (e.g. user
interfaces,  back-end processing, storage, networking) that require
different skill sets to work with.  These different application areas also
require different strategies for testing.  For example, Some application
areas such as graphical user interfaces are notoriously difficult cover
depending on the tools and techniques being used.  These difficulties
usually result from limitations in the way the testing tool interacts with
the application being tested. We wanted to know whether or not
CrashSimulator was able to effectively test the areas of applications its
users were interested in testing.  To answer this question we turn to the
surveys our participants completed regarding their experience with
CrashSimulator.


\subsubsection{Findings}

The results from our surveys indicate that our participants self-reported
backgrounds have an effect on the areas of an application they tested...

Developers with a high degree of experience in software engineering tended
to focus on testing an applications interaction with various library and
operating system provided APIs.  Survery results indicate participants were
satisfied with CrashSimulator's ability to test application's interaction
with file and and network APIs....

Developers with a lower self-reported experience in software engineering
focused more on user interface level concerns...  Preferred AFL's ability
to easily supply a file to be processed to an application....


\subsubsection{Discussion}

Findings indicate that a higher level of software development experience
tends to push participants towards testing interfaces between modules in an
application.  This resulted in overall higher satisfaction with
CrashSimulator amongst participants.....

Users with less self-reported software engineering experience expressed
some concern with CrashSimulator's limitations around testing the user
facing aspects of applications.  For example, CrashSimulator requires more
up front effort than effort than tools like AFL when conducting simple
tests such as mutating the properties of an input file.....


\subsection{Does CrashSimulator allow its users to construct a test suite
efficiently?}

Constructing a test suite using CrashSimulator involves identifying new
anomalies to be tested for and creating the mutators and checkers required
to test an application's response to it.  There is no direct comparison to
this process in AFL...  Mutiny allows users to specify which messages
should be mutated and replayed....


\subsubsection{Findings}

Developers reported a high degree of satisfaction with regard to the speed
of setup for AFL.  Participants felt like CrashSimulator required an
overall higher intitialy outlay of effort before testing could begin.
Additionally, they indicated that expanding a CrashSimulator test suite by
implementing new checkers and mutators was more difficult that either
expanding the scope of fuzzing with AFL or instructing Mutiny to mutate
different messages from a recorded network session.  Participants felt that
CrashSimulator allowed for more specific testing and were pleased with the
ability of CrashSimulator tests to be used across multiple applications
with minimal additional effort once they had been constructed......


\subsubsection{Discussion}

!!!!Discussion!!!!


\subsection{Does CrashSimulator require significant skills to be useful?}

A user's background can dramatically influence their user experience with a
tool.  In answering this question we hope to ascertain whether
CrashSimulator requires its users have significant skills in a particular
area in order to be effective.  Table 2 compares our participants
self-reported skillsets to their positive or negative experiences with
CrashSimulator...


\subsubsection{Findings}

\begin{figure}[t]
  \center{}
  \includegraphics[scale=.5]{images/table2}
  \caption{\emph{Skill Level Compared to User Experience.}}
  \label{fig-skill-exp}
\end{figure}

As can be seen in Table 2, a strong background in operating systems
concepts correlates with a more satisfactory user experience with
CrashSimulator....

Interestingly,  some participants that reported less background in
operating systems had a positive experience with CrashSimulator.  These
participants relied more heavily on built in
anomalies/checkers/mutators....


\subsubsection{Discussion}

Given the level with which CrashSimulator interacts with applications, it
is not surprising that users with significant operating systems backgrounds
are better able to utilize the tool's more advanced features....

It is encouraging to see that users with less os background had success
with the tool.  Based on this, we believe it is likely that CrashSimulator
could be successfully adopted by real-world teams whose members have
diverse background skillsets.  Users that are more comfortable with the
concepts CrashSimulator relies on can supply new testing materials as
needed and other users can rely on CrashSimulators portable nature to use
these tests against new applications without having to worry about their
implementation details.


\subsection{Does CrashSimulator provide output that is useful in locating
and fixing bugs?}


\subsubsection{Findings}

Developer feedback across the board indicated that CrashSimulator's output
made it easier to identify bugs than the simpler crash reports provided by
AFL and Mutiny.  This impression is backed up by the results in Table 3
that show a higher percentage of bugs identified with CrashSimulator were
fixed than bugs identified with the other tools....


\subsubsection{Discussion}

There are two reasons, other than output quality, that bugs found with
CrashSimulator were more likely to be fixed -- bug "depth" and the ease
with which a bug can be fixed.  It is possible that, because CrashSimulator
focuses on a lesser targeted area of an application, more easily fixed, low
hanging fruit style bugs were identified compared to the more battle tested
user input handling codepaths that AFL and Mutiny tend to explore.
Additionally, many of the bugs types identifiable with CrashSimulator arise
because of a minor missing check meaning that the fix is small and easily
incorporated....
